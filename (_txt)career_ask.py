# -*- coding: utf-8 -*-
"""(.txt)career-ask

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W0EOc4vvLP0up_6f8tA-gCE4G963chGE
"""

import streamlit as st
import os
import sys
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.llms import OpenAI
from langchain.chains import ConversationChain
from langchain.prompts import PromptTemplate

from google.colab import files
uploaded = files.upload()

with open("secrets.toml", "r") as file:
    openai.api_key = file.read().strip()
print(openai.api_key)

# Initialize OpenAI embedding model
embeddings = OpenAIEmbeddings()

class templates:
	""" store all prompts templates """
	begin_convo = """ you are an expert career interest test researcher and conductor.we will go through three steps:
              1. i will send you my work experience, degree details.
              2. output 20 most likely occupations based on my experience
              3. you may feel free to ask questions about myself or my background in order to determine my career interest. You may also ask additional questions about my experience to determine which 3 occupations is most likely for me to succeed in.
              4. use those responses to select 3 top occupations
              5. tell me 3 occupations i could take up next with my existing experience.
            {context}

            Question: {question}
            Answer:"""

from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import NLTKTextSplitter
bg = st.text_area("Please enter your background here e.g. age, marriage status, parents' occupation, gradnparents' occupation, where were you born and raised in ()")

'''The variable "bg" was passed to the embeddings function later on.'''

def embeddings(text):
	text_splitter = NLTKTextSplitter()
	texts = text_splitter.split_text(text)
	embeddings = OpenAIEmbeddings()
	docsearch = FAISS.from_texts(texts, embeddings)
	retriever = docsearch.as_retriever(search_tupe='similarity search')
	return retriever

from dataclasses import dataclass
from typing import Literal

@dataclass
class Message:
	origin: Literal["human", "ai"]
	message: str

def initialize_session_state():
    if "retriever" not in st.session_state:
        st.session_state.retriever = embeddings(bg)

    if "chain_type_kwargs" not in st.session_state:
        begin_convo = PromptTemplate(input_variables=["context", "question"],
                                          template=templates.begin_convo)
        st.session_state.chain_type_kwargs = {"prompt": begin_convo}
    # interview history
    if "history" not in st.session_state:
        st.session_state.history = []
        st.session_state.history.append(Message("ai", "Hello there! I am your career genie  today. I will assess your career interests through a series of questions. Let's get started! Please start by saying hello or introducing family background and where you received education. Note: The maximum length of your answer is 4097 tokens!"))

    # token count
    if "token_count" not in st.session_state:
        st.session_state.token_count = 0
    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferMemory()
    if "guideline" not in st.session_state:
        llm = ChatOpenAI(
            model_name="gpt-3.5-turbo",
            temperature=0.1)
        st.session_state.guideline = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type_kwargs=st.session_state.chain_type_kwargs, chain_type='stuff',
            retriever=st.session_state.retriever, memory=st.session_state.memory).run(
            "Create a list of potential occupations and rationale. 3 occupations in total.")

    if "conversation" not in st.session_state:
        llm = ChatOpenAI(
        model_name = "gpt-3.5-turbo",
        temperature = 0.1)
        PROMPT = PromptTemplate(
            input_variables=["history", "input"],
            template=templates.conversation_template)
        st.session_state.conversation = ConversationChain(prompt=PROMPT, llm=llm,
                                                       memory=st.session_state.memory)
    if "feedback" not in st.session_state:
        llm = ChatOpenAI(
        model_name = "gpt-3.5-turbo",
        temperature = 0.1)
        st.session_state.feedback = ConversationChain(
            prompt=PromptTemplate(input_variables = ["history", "input"], template = templates.feedback_template),
            llm=llm,
            memory = st.session_state.memory
        )

# submit job description
bg = st.text_area("Please enter your family and education background: ")
# auto play audio
auto_play = st.checkbox("Let Career Genie speak! (Please don't switch when the magic happens)")

if bg:
  # initialize session states
  initialize_session_state()
  # feedback requested button
  feedback = st.button("Get Interview Feedback")

  token_placeholder = st.empty()
  chat_placeholder = st.container()
  answer_placeholder = st.container()
else:
    st.info("Please enter your family and education background to start the interview.")

# Page title
st.set_page_config(page_title='ðŸ¦œðŸ”— Career Ask')
st.title('ðŸ¦œðŸ”— Career Ask')

